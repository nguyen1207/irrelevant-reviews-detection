{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import Trainer, TrainingArguments, DefaultDataCollator, PretrainedConfig, PreTrainedModel, DataCollatorWithPadding\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from peft import LoraConfig, LoftQConfig, TaskType, get_peft_model\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5DataLoader:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        dataset = load_dataset(\"csv\", data_files=\"../../data/temp_data.csv\", split='train')\n",
    "        dataset = dataset.class_encode_column('label')\n",
    "        dataset = dataset.train_test_split(test_size=0.2, stratify_by_column='label')\n",
    "        self.train_dataset, self.eval_dataset = dataset['train'], dataset['test']\n",
    "        self.train_dataset.set_transform(self._transform)\n",
    "        self.eval_dataset.set_transform(self._transform)\n",
    "        \n",
    "    \n",
    "    def _transform(self, examples):\n",
    "        docs = [f'passage: {doc}' for doc in examples['description']]\n",
    "        queries = [f'query: {query}' for query in examples['comment']]\n",
    "\n",
    "        batch_dict = self.tokenizer(queries, \n",
    "                                    text_pair=docs,\n",
    "                                    max_length=512,\n",
    "                                    truncation=True,\n",
    "                                    )\n",
    "        \n",
    "        batch_dict['label'] = examples['label']\n",
    "        \n",
    "        return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5NNTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(E5NNTrainer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5NNConfig(PretrainedConfig):\n",
    "    model_type = 'E5_NN'\n",
    "    \n",
    "    def __init__(self, num_labels=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "\n",
    "class E5NN(PreTrainedModel):\n",
    "    config_class = E5NNConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(E5NN, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-large', load_in_8bit=True)\n",
    "        self.linear = nn.Linear(1024, config.num_labels, dtype=torch.qint8)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels, **kwargs):\n",
    "        e5_outputs = self.e5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        features = e5_outputs.pooler_output\n",
    "        logits = self.linear(features)\n",
    "        prob = torch.softmax(logits, dim=-1).to(device)\n",
    "        loss = self.cross_entropy(prob, labels)\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5NNCollator(DataCollatorWithPadding):\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch_dict = {\n",
    "            'input_ids': [example['input_ids'] for example in examples],\n",
    "            'attention_mask': [example['attention_mask'] for example in examples],\n",
    "            'labels': [int(example['label']) for example in examples]\n",
    "        }\n",
    "        \n",
    "\n",
    "        collated_batch_dict = self.tokenizer.pad(\n",
    "            batch_dict,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors\n",
    "        )\n",
    "\n",
    "        return collated_batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "data_loader = E5DataLoader(tokenizer)\n",
    "train_data = data_loader.train_dataset\n",
    "eval_data = data_loader.eval_dataset\n",
    "\n",
    "loftq_config = LoftQConfig(loftq_bits=8)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS if device == 'cuda' else None,\n",
    "                         init_lora_weights=\"loftq\" if device == 'cuda' else dict(),\n",
    "                         loftq_config=loftq_config,\n",
    "                         target_modules=[\n",
    "                           'query',\n",
    "                           'key'\n",
    "                         ],\n",
    "                         inference_mode=False, \n",
    "                         r=8, \n",
    "                         lora_alpha=32, \n",
    "                         lora_dropout=0.1\n",
    "                         )\n",
    "\n",
    "\n",
    "config = E5NNConfig()\n",
    "model = E5NN(config)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "data_collator = E5NNCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  logits = torch.tensor(eval_pred.predictions, device=device)\n",
    "  labels = torch.tensor(eval_pred.label_ids, device=device, dtype=torch.int32)\n",
    "  probs = torch.softmax(logits, dim=-1).to(device)\n",
    "  predictions = probs.argmax(dim=-1).to(device)\n",
    "  metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "  return metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='saved_models/e5nn',\n",
    "    evaluation_strategy='steps',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_strategy='steps',\n",
    "    save_steps=0.2,\n",
    "    logging_steps=0.2,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = E5NNTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
