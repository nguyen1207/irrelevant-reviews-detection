{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from peft import LoftQConfig, LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (AutoModel, AutoTokenizer, DataCollatorWithPadding,\n",
    "                          PretrainedConfig, PreTrainedModel, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5DataLoader:\n",
    "    def __init__(self, tokenizer, data_file):\n",
    "        self.tokenizer = tokenizer\n",
    "        dataset = load_dataset(\"csv\", data_files=data_file, split='train')\n",
    "        dataset = dataset.class_encode_column('label')\n",
    "        dataset = dataset.train_test_split(test_size=0.2, stratify_by_column='label')\n",
    "        self.train_dataset, self.eval_dataset = dataset['train'], dataset['test']\n",
    "        self.train_dataset.set_transform(self._transform)\n",
    "        self.eval_dataset.set_transform(self._transform)\n",
    "\n",
    "\n",
    "    def _transform(self, examples):\n",
    "        docs = [f'passage: {doc}' for doc in examples['description']]\n",
    "        queries = [f'query: {query}' for query in examples['comment']]\n",
    "\n",
    "        assert len(docs) == len(queries)\n",
    "        assert len(queries) == len(examples['label'])\n",
    "\n",
    "        query_batch_dict = self.tokenizer(queries,\n",
    "                                    max_length=512,\n",
    "                                    truncation=True,\n",
    "                                    )\n",
    "\n",
    "        doc_batch_dict = self.tokenizer(docs,\n",
    "                                    max_length=512,\n",
    "                                    truncation=True,\n",
    "                                    )\n",
    "\n",
    "        merged_batch_dict = {f'q_{k}': v for k, v in query_batch_dict.items()}\n",
    "        for k, v in doc_batch_dict.items():\n",
    "            k = f'd_{k}'\n",
    "            merged_batch_dict[k] = v\n",
    "\n",
    "        merged_batch_dict['label'] = examples['label']\n",
    "\n",
    "        return merged_batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5NNTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(E5NNTrainer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5NNConfig(PretrainedConfig):\n",
    "    model_type = 'E5NN'\n",
    "\n",
    "    def __init__(self, num_labels=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "\n",
    "class E5NN(PreTrainedModel):\n",
    "    config_class = E5NNConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(E5NN, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.e5 = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n",
    "        self.linear = nn.Linear(1024*2, config.num_labels)\n",
    "\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels, **kwargs):\n",
    "        e5_outputs = self.e5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        features = e5_outputs.pooler_output\n",
    "        features = torch.cat((features[:len(input_ids)//2], features[len(input_ids)//2:]), dim=-1)\n",
    "        logits = self.linear(features)\n",
    "        prob = torch.softmax(logits, dim=-1).to(device)\n",
    "        loss = self.cross_entropy(prob, labels)\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5NNCollator(DataCollatorWithPadding):\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        q_prefix, d_prefix = 'q_', 'd_'\n",
    "\n",
    "        queries = [{k[len(q_prefix):]: v for k, v in example.items() if q_prefix in k}\n",
    "                        for example in examples]\n",
    "\n",
    "        docs = [{k[len(d_prefix):]: v for k, v in example.items() if d_prefix in k}\n",
    "                        for example in examples]\n",
    "\n",
    "        batch_collated = self.tokenizer.pad(\n",
    "            queries + docs,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors\n",
    "        )\n",
    "\n",
    "        batch_collated['labels'] = torch.tensor([example['label'] for example in examples])\n",
    "\n",
    "        return batch_collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 560,680,962 || trainable%: 0.14026372452432226\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): E5NN(\n",
      "      (e5): XLMRobertaModel(\n",
      "        (embeddings): XLMRobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 1024)\n",
      "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): XLMRobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-23): 24 x XLMRobertaLayer(\n",
      "              (attention): XLMRobertaAttention(\n",
      "                (self): XLMRobertaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): XLMRobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): XLMRobertaIntermediate(\n",
      "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): XLMRobertaOutput(\n",
      "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): XLMRobertaPooler(\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=2048, out_features=2, bias=True)\n",
      "      (cross_entropy): CrossEntropyLoss()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "data_file = '../../data/temp_data.csv'\n",
    "data_loader = E5DataLoader(tokenizer,data_file=data_file)\n",
    "train_data = data_loader.train_dataset\n",
    "eval_data = data_loader.eval_dataset\n",
    "\n",
    "loftq_config = LoftQConfig(loftq_bits=8)\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS if device == 'cuda' else None,\n",
    "                         init_lora_weights=\"loftq\" if device == 'cuda' else dict(),\n",
    "                         loftq_config=loftq_config,\n",
    "                         target_modules=[\n",
    "                           'query',\n",
    "                           'key'\n",
    "                         ],\n",
    "                         inference_mode=False,\n",
    "                         r=8,\n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.1\n",
    "                         )\n",
    "\n",
    "\n",
    "config = E5NNConfig()\n",
    "model = E5NN(config)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "print(model)\n",
    "\n",
    "data_collator = E5NNCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  logits = torch.tensor(eval_pred.predictions, device=device)\n",
    "  labels = torch.tensor(eval_pred.label_ids, device=device, dtype=torch.int32)\n",
    "  probs = torch.softmax(logits, dim=-1).to(device)\n",
    "  predictions = probs.argmax(dim=-1).to(device)\n",
    "  metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "  return metrics.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='saved_models/e5nn',\n",
    "    evaluation_strategy='steps',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    save_strategy='steps',\n",
    "    save_steps=0.2,\n",
    "    logging_steps=0.2,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=['labels']\n",
    ")\n",
    "\n",
    "trainer = E5NNTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57f79203e7746d6b7e97a16a84a9f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.599, 'learning_rate': 1.5675675675675676e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622c633f55da44cdb701d44392d2c893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5616773366928101, 'eval_accuracy': 0.9459459459459459, 'eval_f1': 0.9722222222222222, 'eval_precision': 0.9459459459459459, 'eval_recall': 1.0, 'eval_runtime': 12.8871, 'eval_samples_per_second': 2.871, 'eval_steps_per_second': 0.776, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyen/anaconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5966, 'learning_rate': 1.1351351351351352e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fbd6a1dd9f4f079cf22dab9ac48929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5606305599212646, 'eval_accuracy': 0.9459459459459459, 'eval_f1': 0.9722222222222222, 'eval_precision': 0.9459459459459459, 'eval_recall': 1.0, 'eval_runtime': 15.6901, 'eval_samples_per_second': 2.358, 'eval_steps_per_second': 0.637, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyen/anaconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5904, 'learning_rate': 7.027027027027028e-06, 'epoch': 0.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc370957c32d4c9ea76a0f5ab359db91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5593210458755493, 'eval_accuracy': 0.9459459459459459, 'eval_f1': 0.9722222222222222, 'eval_precision': 0.9459459459459459, 'eval_recall': 1.0, 'eval_runtime': 12.0806, 'eval_samples_per_second': 3.063, 'eval_steps_per_second': 0.828, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyen/anaconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5906, 'learning_rate': 2.702702702702703e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5440e9d982fd493baa2f1e321c79592a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5590757727622986, 'eval_accuracy': 0.9459459459459459, 'eval_f1': 0.9722222222222222, 'eval_precision': 0.9459459459459459, 'eval_recall': 1.0, 'eval_runtime': 13.4099, 'eval_samples_per_second': 2.759, 'eval_steps_per_second': 0.746, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyen/anaconda3/envs/py311/lib/python3.11/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.2584, 'train_samples_per_second': 0.946, 'train_steps_per_second': 0.24, 'train_loss': 0.5962875340435956, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=37, training_loss=0.5962875340435956, metrics={'train_runtime': 154.2584, 'train_samples_per_second': 0.946, 'train_steps_per_second': 0.24, 'train_loss': 0.5962875340435956, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
