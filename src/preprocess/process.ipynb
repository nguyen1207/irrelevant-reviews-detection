{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'fashion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(f\"../../data/raw_data/reviews/{name}\", \"*.csv\"))\n",
    "\n",
    "reviews_df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.read_csv(f'../../data/product_{name}.csv', encoding='utf8')\n",
    "products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = products_df.merge(reviews_df, 'inner', left_on=['product_id', 'shop_id'], right_on=['product_id', 'shop_id'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.duplicated()]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def replace_all(text):\n",
    "    text = str(text)\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "\n",
    "EMOJI_PATTERN = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\" \n",
    "    \"]+\"\n",
    ")\n",
    "\n",
    "chunk_count = 1000000\n",
    "num_of_saves = 50\n",
    "file_count = 0\n",
    "\n",
    "for i in tqdm(range(0, df.shape[0], chunk_count)):\n",
    "    chunk = df.loc[i:i+chunk_count]\n",
    "\n",
    "    # remove emojis\n",
    "    chunk['description'] = chunk['description'].str.replace(EMOJI_PATTERN, '', regex=True)\n",
    "\n",
    "\n",
    "    # remove consecutive blanks\n",
    "    chunk['description'] = chunk['description'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # remove consecutive new lines\n",
    "    chunk['description'] = chunk['description'].str.replace(r'\\n', ' ', regex=True)\n",
    "    chunk['description'] = chunk['description'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "    # remove hashtags\n",
    "    chunk['description'] = chunk['description'].str.replace(\"(#\\w+\\s*)+\", ' ', regex=True)\n",
    "\n",
    "    # normalize unicode\n",
    "    chunk['description'] = chunk['description'].str.normalize('NFKD')\n",
    "\n",
    "    # normalize vietnamese tone\n",
    "    chunk['description'] = chunk['description'].map(replace_all)\n",
    "    \n",
    "    chunk['comment'] = chunk['comment'].str.replace(EMOJI_PATTERN, '', regex=True)\n",
    "    chunk['comment'] = chunk['comment'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    chunk['comment'] = chunk['comment'].apply(lambda x: x.strip())\n",
    "    chunk['comment'] = chunk['comment'].str.replace(r'\\n', ' ', regex=True)\n",
    "    chunk['comment'] = chunk['comment'].str.replace(\"(#\\w+\\s*)+\", ' ', regex=True)\n",
    "    chunk['comment'] = chunk['comment'].str.normalize('NFKD')\n",
    "    chunk['comment'] = chunk['comment'].map(replace_all)\n",
    "    \n",
    "    chunk['product_id'] = chunk['product_id'].astype(str)\n",
    "    chunk['shop_id'] = chunk['shop_id'].astype(str)\n",
    "    \n",
    "    chunk['length'] = chunk['description'].str.count(' ') + df['comment'].str.count(' ') + 2\n",
    "    chunk['length'] = chunk['length'].astype(int)\n",
    "    \n",
    "    chunk_length_less_512 = chunk[chunk['length'] <= 512]\n",
    "    \n",
    "    reviews_count_chunk = chunk_length_less_512.groupby(['product_id', 'shop_id'])['comment'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "    more_than_20_less_than_50_ids = reviews_count_chunk[(reviews_count_chunk['count'] >= 20) & (reviews_count_chunk['count'] <= 50)]\n",
    "    more_than_20_less_than_50 = chunk_length_less_512.merge(more_than_20_less_than_50_ids, 'inner', left_on=['product_id', 'shop_id'], right_on=['product_id', 'shop_id'])\n",
    "    \n",
    "    for chunk in np.array_split(more_than_20_less_than_50, num_of_saves):\n",
    "        chunk.to_csv(f'./data/merge/{name}/{name}_chunk_{file_count}.csv', encoding='utf-8-sig', index=False)\n",
    "        file_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_20_less_than_50 = df_length_less_512.merge(more_than_20_less_than_50_ids, 'inner', left_on=['product_id', 'shop_id'], right_on=['product_id', 'shop_id'])\n",
    "more_than_20_less_than_50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
